# LLMs Architecture 
![Alt Text](images/Transformer.png)


This repository contains implementations of **LLMs**
---

Implemented Architectures
1. **GPT2**
2. **BERT**
3. **Llama**

## ğŸ“š Description  

This repository is structured as follows:
1. **Data** Folder contains small text files for simple training 
2. **GPT2** Folder contains training, model, textsampler 
3. **BERT** Folder contains Bert Model and Bert Modules
4. **Llama** Folder contains Llama Model with GQA and RoPE Modules  

---

## ğŸš€ Goals  
- Implement and experiment with different model architecture. 
- Develop foundational components for future research in **transformers**.
- Provide clean and modular code.

---

## ğŸ“ Research Papers Implemented (or To Be Implemented)
1. **GPT2 Paper** (https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)
2. **Bert Paper** (https://arxiv.org/abs/1810.04805)
3. **Llama Paper** (https://arxiv.org/abs/2307.09288) 

---

## ğŸ’¡ Future Work  
Implement more LLMs architectures.

## â­ Contributions
Feel free to contribute to this repository or suggest improvements.